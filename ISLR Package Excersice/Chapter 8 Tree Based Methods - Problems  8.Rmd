---
title: "Chapter 8 Tree Based Methods - Problems  8"
author: "kartheek raj mulasa"
date: "1/16/2020"
output:
  pdf_document: default
  html_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## 8 In the lab, a classiﬁcation tree was applied to the Carseats data set after converting Sales into a qualitative response variable. Now we will seek to predict Sales using regression trees and related approaches, treating the response as a quantitative variable.

### (a) Split the data set into a training set and a test set.

Data pulling from ISLR library and data snapshot.
```{r carseats}

require(MASS)
require(tree)
require(ISLR)
require(randomForest)
library(MASS)
library(tree)
library(ISLR)
library(randomForest)


carseats<-data.frame(Carseats)
head(carseats)
str(carseats)
summary(carseats)
```

Test and train data splits 

```{r carseats test & train dara split}
set.seed(1)
sd<-sample(1:nrow(carseats), round(nrow(carseats)/2))
train<-carseats[sd,]
test<-carseats[-sd,]


```


### (b) Fit a regression tree to the training set. Plot the tree, and interpret the results. What test MSE do you obtain?

Answer

Building a random forest regression model to train.

```{r randomforestmodel1}


ctree<-tree(Sales~.,data = train)
par(mfrow=c(1,1),bg="lightyellow")
plot(ctree,col="red",lwd=2)
text(ctree, pretty=0)
summary(ctree)


```

Predicting the results and MSE on test data.

````{r predict}

 TESTMSE<-function(x) {
  h=x
  predictvalues<-predict(h,newdata=test)
  mean((predictvalues-test$Sales)^2)
}
TESTMSE(ctree)
```

### (c) Use cross-validation in order to determine the optimal level of tree complexity. Does pruning the tree improve the test MSE?

Answer 

```{r Cross validation}
set.seed(5)
cv<-cv.tree(ctree)
par(mfrow=c(1,2),bg="lightyellow")
plot(cv$size,cv$dev,type="b",col="darkred",lwd=2)
plot(cv$k,cv$dev,type="b",col="darkgreen",lwd=2)

```

Insight3 : Lowest deviance at the size of 17 is achieved by above plots 

Apply this size and prune to tree 

```{r prune}
set.seed(1)
pcv<-prune.tree(ctree,best = 17)
par(mfrow=c(1,1),bg="lightyellow")
plot(pcv,col="red",lwd=2)
text(pcv,pretty=0)
TESTMSE(pcv)
```

Insight4: MSE slightly reduced after prune the model  from 4.9 to 4.8.

###  (d)  Use the bagging approach in order to analyze this data. What test MSE do you obtain? Use the importance() function to determine which variables are most important.

```{r bagging}
bc<-randomForest(Sales~.,data=train,mtry=10,importance=T)
TESTMSE(bc)
bc$importance

```
Insight 4: MSE lowered from 4.8 to 2.6 by apply bagging.Price,Shelveloc,Compprice and age in that order are top importance variables to predict the Sales.


### (e) Use random forests to analyze this data. What test MSE do you obtain? Use the importance() function to determine which variables are most important. Describe the eﬀect of m, the number of variables considered at each split, on the error rate obtained.


```{r m trials}
set.seed(2)
mtrail<- function(x){
h=x
carrftree<-randomForest(Sales~.,data=train,mtry=h,importance=T)
TESTMSE(carrftree)

}

for (i in 1:10){
  mtrail(i)
  print(mtrail(i))
  
}

```

Insight 5: as m number increases Test MSE values decreases till m=9 the next m value plugged the Test MSE increases.Best fit in this instance 9 varaiables to predict the Sales.


```{r m trials1}

carrftree<-randomForest(Sales~.,data=train,mtry=9,importance=T)
carrftree$importance
par(mfrow=c(1,1),bg="lightyellow")
varImpPlot(carrftree,col="red",lwd=2)

```

Same as insight 4 i.e Price,Shelveloc,Compprice and age in that order are top importance variables to predict the Sales.
